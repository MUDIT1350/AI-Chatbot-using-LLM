# -*- coding: utf-8 -*-
"""INTERNET RESEARCH BOT FROM MUDIT SHANKER SAXENA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wt1-59Z4FYStux3HhBEfyQNjAnQIMdy5
"""

import requests
from googlesearch import search
from bs4 import BeautifulSoup
import json
import time
import re
from urllib.parse import urlparse
import warnings
warnings.filterwarnings('ignore')

class InternetResearchBot:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def search_google(self, query, num_results=5):
        """Search Google for relevant information"""
        print(f"ðŸ” Searching for: {query}")
        results = []

        try:
            for url in search(query, num_results=num_results, advanced=True):
                results.append({
                    'title': url.title,
                    'url': url.url,
                    'description': url.description
                })
        except Exception as e:
            print(f"âš ï¸ Google search failed: {e}")
            # Fallback to DuckDuckGo HTML
            return self.search_duckduckgo(query, num_results)

        return results

    def search_duckduckgo(self, query, num_results=5):
        """Fallback search using DuckDuckGo"""
        try:
            url = "https://html.duckduckgo.com/html/"
            params = {'q': query}
            response = self.session.post(url, data=params)
            soup = BeautifulSoup(response.text, 'html.parser')

            results = []
            for result in soup.find_all('a', class_='result__url', limit=num_results):
                title = result.find_next('h2')
                snippet = title.find_next('a', class_='result__snippet')
                if title and snippet:
                    results.append({
                        'title': title.text.strip(),
                        'url': "https://" + result.text.strip(),
                        'description': snippet.text.strip()[:200]
                    })
            return results
        except:
            return []

    def fetch_page_content(self, url):
        """Fetch and extract content from a webpage"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # Remove script and style elements
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()

            # Get text
            text = soup.get_text()

            # Clean up text
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)

            return text[:3000]  # Limit to first 3000 chars

        except Exception as e:
            print(f"âš ï¸ Failed to fetch {url}: {e}")
            return None

    def extract_key_info(self, text, query):
        """Extract relevant information based on the query"""
        if not text:
            return ""

        # Simple relevance scoring by looking for query terms
        query_terms = set(query.lower().split())
        sentences = re.split(r'[.!?]', text)

        relevant_sentences = []
        for sentence in sentences:
            sentence_lower = sentence.lower()
            score = sum(1 for term in query_terms if term in sentence_lower)
            if score > 0:
                relevant_sentences.append((score, sentence.strip()))

        # Sort by relevance score
        relevant_sentences.sort(key=lambda x: x[0], reverse=True)

        # Take top 5 most relevant sentences
        top_sentences = [s[1] for s in relevant_sentences[:5]]

        return " ".join(top_sentences)

    def get_wikipedia_summary(self, query):
        """Get summary from Wikipedia"""
        try:
            url = "https://en.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'format': 'json',
                'titles': query,
                'prop': 'extracts',
                'exintro': True,
                'explaintext': True,
            }

            response = self.session.get(url, params=params)
            data = response.json()

            pages = data['query']['pages']
            for page_id in pages:
                extract = pages[page_id].get('extract', '')
                if extract:
                    return extract[:1000]  # Limit length

        except Exception as e:
            print(f"âš ï¸ Wikipedia API error: {e}")

        return None

    def research_query(self, user_query):
        """Main research function"""
        print(f"\nðŸ“ User Query: {user_query}")
        print("=" * 50)

        # First try Wikipedia for factual queries
        if any(word in user_query.lower() for word in ['what is', 'who is', 'definition', 'explain']):
            wiki_result = self.get_wikipedia_summary(user_query)
            if wiki_result:
                print("ðŸ“š Found Wikipedia summary")
                return self.generate_answer(user_query, wiki_result, "Wikipedia")

        # Search the web
        search_results = self.search_google(user_query, num_results=3)

        if not search_results:
            return "Sorry, I couldn't find any information on that topic."

        all_content = []
        sources = []

        for i, result in enumerate(search_results, 1):
            print(f"\nðŸ“„ Source {i}: {result['title']}")
            print(f"ðŸ”— URL: {result['url']}")

            content = self.fetch_page_content(result['url'])
            if content:
                key_info = self.extract_key_info(content, user_query)
                if key_info:
                    all_content.append(key_info)
                    sources.append({
                        'title': result['title'],
                        'url': result['url'],
                        'snippet': key_info[:200] + "..."
                    })

            time.sleep(1)  # Be polite to servers

        if not all_content:
            return "I found some sources but couldn't extract relevant information."

        combined_content = " ".join(all_content)
        return self.generate_answer(user_query, combined_content, sources)

    def generate_answer(self, query, content, sources):
        """Generate a coherent answer from the collected information"""
        print("\nðŸ¤– Generating answer...")
        print("-" * 50)

        # Process and summarize
        answer = self.summarize_content(content, query)

        # Format the answer nicely
        formatted_answer = f"**Question:** {query}\n\n"
        formatted_answer += f"**Answer:** {answer}\n\n"

        if isinstance(sources, list):
            formatted_answer += "**Sources:**\n"
            for i, source in enumerate(sources, 1):
                formatted_answer += f"{i}. {source['title']}\n"
                formatted_answer += f"   Snippet: {source['snippet']}\n"
        elif sources:
            formatted_answer += f"**Source:** {sources}\n"

        return formatted_answer

    def summarize_content(self, content, query):
        """Simple summarization algorithm"""
        # Split into sentences
        sentences = re.split(r'[.!?]', content)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]

        # Simple scoring based on query relevance and position
        query_terms = query.lower().split()
        scored_sentences = []

        for i, sentence in enumerate(sentences):
            score = 0

            # Score by query term matches
            sentence_lower = sentence.lower()
            for term in query_terms:
                if len(term) > 3:  # Ignore short words
                    score += sentence_lower.count(term) * 2

            # Score by position (earlier sentences often more important)
            score += max(0, (len(sentences) - i) / len(sentences)) * 0.5

            # Score by length (medium-length sentences often best)
            if 50 < len(sentence) < 200:
                score += 1

            scored_sentences.append((score, sentence))

        # Sort by score and take top sentences
        scored_sentences.sort(key=lambda x: x[0], reverse=True)
        top_sentences = [s[1] for s in scored_sentences[:3]]

        return " ".join(top_sentences)


def main():
    """Main interactive loop"""
    print("=" * 60)
    print("ðŸŒ INTERNET RESEARCH BOT FROM MUDIT SHANKER SAXENA")
    print("=" * 60)
    print("MUDIT SHANKER SAXENA can search the web to answer your questions!")
    print("Type 'quit', 'exit', or 'bye' to end the conversation.\n")

    bot = InternetResearchBot()

    while True:
        try:
            user_input = input("\nYou: ").strip()

            if user_input.lower() in ['quit', 'exit', 'bye', 'q']:
                print("\nðŸ‘‹ Goodbye! Have a great day!")
                break

            if not user_input:
                print("Please enter a question.")
                continue

            # Check if it's a question
            if not any(marker in user_input.lower() for marker in ['?', 'what', 'how', 'why', 'when', 'where', 'who', 'explain', 'tell me']):
                print("Hint: Try asking a question (e.g., 'What is...', 'How does...')")
                continue

            print("\n" + "=" * 60)
            answer = bot.research_query(user_input)
            print("\n" + "=" * 60)
            print("\n" + answer)

        except KeyboardInterrupt:
            print("\n\nðŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"\nâš ï¸ An error occurred: {e}")
            print("Let's try another question!")


if __name__ == "__main__":
    # Install required packages if not present
    try:
        import requests
        from googlesearch import search
        from bs4 import BeautifulSoup
    except ImportError:
        print("Installing required packages...")
        import subprocess
        import sys

        packages = ['requests', 'beautifulsoup4', 'google']
        subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)

        print("\nâœ… Packages installed! Restarting...")
        import requests
        from googlesearch import search
        from bs4 import BeautifulSoup

    main()